261 Machine Learning Fundamentals Course Instruct eena Sharma, Ph.D.

Bachelor of Science (Honours) in Data Science and Artificial Intelligence

DA261: Machine Learning Fundamentals

Instructor: Teena Sharma, Ph.D.
Mehta Family School of Data Science and Artificial Intelligence,
Indian Institute of Technology Guwahati, India

Optional Note: Usage of this material for teaching and education is considered fair use. However, if that
includes posting images to a website, or commercial usage that could be considered copyright infringement.
In such case, you are required to reach out to the author(s) and the Institute for permission before the usage.


Learning Objectives

01 Understand the fundamentals of decision tree.

02 Structure and components of a decision tree.

03 Understand and apply the ID3 and CART algorithms.

04 Calculate entropy, information gain, and Gini index for decision-making.

05 Construct decision trees for classification problerns using real-world datasets.
06 Hyperplanes

07 Maximal margin classifier

08 Support vector classifier

09 Support vector machine

What Are Decision Trees?

DA261 Machine Learning Fundamentals Course Instructor: Teena Sharma, Ph.D.

e Definition: A supervised learning algorithm for Decision Tee Diagram

classification and regression, using a tree-like oot Node
structure to make decisions.

Max depth

Key Idea: Sequentially asks questions about data 2

features to predict a class label. sie low strong Weak

Applications: Medical diagnosis (e.g., diabetes
detection), recommendation systems, gaming
(e.g., XBox Kinect body tracking).

Fig-1: Simple decision tree for "Play Tennis"

re: https://github.com/milaan9/Python Decision Tree_and Random Forest


Why Use Decision Trees?

e Easy to interpret and visualize.

e Handles numerical and categorical data.
e No data scaling required.

e Manages missing values effectively.

e Non-parametric: No assumptions about data distribution.


Components of a Decision Tree

e Root Node: Entire dataset, starting point.
e Internal Nodes: Features/attributes (e.g., petal length).
e Branches: Decision rules (e.g., petal length <= 2.5).

e Leaf Nodes: Class labels (e.g., setosa).

Terminology

e Decision Nodes: Where splits occur based on features.
e Chance Nodes: Probabilistic outcomes at splits.

e Terminal Nodes: Final predictions (leaf nodes).

e Glossary:

o Parent Node: Node that splits into child nodes.
o Child Node: Resulting nodes from a split.

How Decision Trees Work

¢ Select the best feature using a criterion (¢.g., Pseudocode

information gain, Gini impurity).
e Split data into subsets based on the feature.
e Recurse until stopping criteria are met.

function build_tree(data, features):
if all data have same label or no features left:
return leaf node with majority label
else:
find best feature to split
split data into subsets

Tree

Root Node for each subset:
Branches Oy, build subtree recursively
umes wate searainove return tree node with feature and subtrees
Branches Yes: o No Yes” 3 No.
Leat Node Leaf Node: Leaf Node: Loaf Node:

Src: https://medium.com/@jainvidip/understanding-decision-trees-1ba0efSf6bb4


Types of Decision Trees

e Classification Trees: Predict discrete outcomes (e.g., spam/not spam).
e Regression Trees: Predict continuous outcomes (e.g., house prices).

e Inthis course we will focus on Classification trees.

Type Outcome Type Example Use Case
Classification Discrete Spam detection
Regression Continuous Predicting house prices


Another Classification Idea

e We learned about linear classification (e.g., logistic regression), and nearest neighbors. Any
other idea?

e Pick an attribute, do a simple test
e Conditioned on a choice, pick another attribute, do another test
e Inthe leaves, assign a class with a majority vote

e Do other branches as well


DA261 Machine Learning Fundamentals Course Instructor: Teena Sharma, Ph.D.

Decision Tree: Example

Is Person fit or unfit?

Age < 30
No

Unfit Fit Fit Unfit


DA261 Machine Learning Fundamentals Course Instructor: Teena Sharma, Ph.D.

Decision Tree: Classification

Test example

Person = unfit
Age < 30 Yes

Age < 30
No


Types of Decision Tree Algorithms:

The popular types of decision tree algorithms are:
1. IDS

2. CART (Classification And Regression Trees)

ID3 (Iterative Dichotomiser 3)

e 1D3 decision tree algorithm uses Information Gain to decide the splitting points.

e Inorder to measure how much information we gain, we can use Entropy to calculate the
homogeneity of a sample.

What is “Entropy”?
e It is a measure of the amount of uncertainty in a data set.
e Entropy controls how a Decision Tree decides to split the data.
e It actually affects how a Decision Tree draws its boundaries.


Effect of Entropy

k
HQ) =— DPR =x)logsPOX = x) :
i=1

More uncertainty, more entropy! 9
S05
x

Information Theory interpretation:

H(X) is the expected number of bits needed to encode a

randomly drawn value of Y (under most efficient code) 0

Src: https://en.wikipedia.org/wiki/Information_gain_(decision_tree)


Effect of Entropy

High Entropy:
e Variable has a uniform like distribution
e Flat histogram
e Values sampled from it are less predictable

Low Entropy:
e Xis from a varied (peaks and valleys)
e Histogram has many lows and highs
e Values sampled from it are more predictable

ID3 Algorithm

1. Compute the entropy for data-set Entropy

a. Calculate Entropy (Amount of uncertainty in dataset) using following formula:

H(S) = ~ Xxex p(«) logs p(x) or Entropy(S) = —F loga(5—) — 57 loga(5)

S- The current dataset for which entropy is being calculated

p(x)- The proportion of the number of elements in class X to the number of elements
inset S

p - number of positive elements in the dataset

n- number of negative elements in the dataset


ID3 Algorithm

2. For every attribute/feature:
a. Calculate Entropy for all other values Entropy(A)
b. Take average Information Entropy for the current attribute

i. Calculate Average Information

I (Attribute) = eres p+ “1. Entropy(A)

p - number of positive elements in the dataset

n- number of negative elements in the dataset

pi — number of positive elements for the i‘ value of the attribute

n; — number of negative elements for the i value of the attribute
Entropy(A) — entropy of subset A formed by the i*value of the attribute


ID3 Algorithm

c. Calculate gain for the current attribute
i. Calculate Information Gain: (Difference in Entropy before and after splitting

dataset on attribute A)

Gain = Entropy(S) - I(Attribute)

3. Pick the Highest Gain Attribute

4. Repeat until we get the tree we desire


261 Machine Learning Fundamentals

ID3 Algorithm: Example

eena Sharma, Ph.D.

Outlook Temperature | Humidity | Wind PlayGolf Decision Tree Diagram

Sunny Hot High Weak No

Sunny Hot High Strong | No Root Node

Overcast Hot High Weak Yes

Rainy Mild High Weak Yes

Rainy Cool Normal Weak Yes

Rainy Cool Normal Strong | No Overcast

Overcast | Cool Normal__| Strong | Yes | Max depth
Sunny Mild High Weak No =2
Sunny Cool Normal Weak Yes

Rainy Mild Normal Weak Yes High Low Strong Weak

Sunny Mild Normal Strong | Yes

Overcast Mild High Strong | Yes

Overcast Hot Normal Weak Yes

Rainy Mild High Strong | No


CART (Classification and Regression Tree)

* 1D3 decision tree algorithm uses Information Gain ration to decide the splitting
points.

* CART algorithm uses a new metric named Gini index to create decision points
for classification tasks. It uses Gini method to create split points including Gini

Index (Gini Impurity) and Gini Gain.

20

CART (Classification and Regression Tree)

What is “Gini Index”?
® Gini index is a metric for classification tasks in CART. It stores sum of squared probabilities of

each class.

a1

Steps to calculate Gini Index

1. Compute the Gini Index for each attribute/feature values: Gini (Attribute='value)

Gini (Attribute = "value") = Gini(Av) = 1— Xp?
where i=1 to the number of classes
2. Compute the weighted sum of Gini Indexes for attribute/feature Gini (Attribute)
Gini (Attribute) = Yip, * Gini(Av)
where v = values of attribute/features
3. Pick the Lowest Gini Index Attribute.

4. Repeat until we get the tree we desired.

22

Comparison between ID3 and CART

Training Algorithm CART (Classification and Regression | ID3 (Iterative Dichotomiser 3)
Tree)

Target(s) Classification and Regression Classification

Metric Gini Index Entropy function and

Information gain

Constant Function Select its splits to achieve the subsets Yield the largest Information
(Based on what to split?) | that minimize Gini Impurity Gain for categorical targets

23


Support Vector Machines

Support vector machine (SVM) is a supervised method for binary classification (two-class). It is a
generalization of 1 and 2 below.

1. Maximal Margin Classifier: A classifier that finds a separating hyperplane for linearly separable data
with the maximum margin (widest separation} between classes. This is only applicable when the data
is perfectly linearly separable (no misclassification)

2. Support Vector Classifier: An extension of the maximal margin idea to handle data that is not
perfectly separable. It allows some classification errors or points within the margin (using slack
variables, discussed later) but still finds a linear decision boundary (hyperplane). This is essentially
the soft-margin linear SVM concept.

3. Support Vector Machine (General case): Further generalizes the classifier to allow non-linear decision
boundaries by using the kernel trick (mapping data to higher dimensions). In essence, an SVM can
create non-linear boundaries in the original feature space by finding a linear separator in a
transformed high-dimensional space.

24


History of SVM

Origins (1960s):
+ Introduced by Vladimir Vapnik and Alexey Chervonenkis in 1963.
+ Initial concept focused on linear separation of data with maximum margin.

Development (1990s):
+ In 1992, Vapnik and colleagues extended SVM to non-linear cases using the kernel trick.

« Soft margin classification introduced to handle noisy data (1995).

Adoption:
* Gained popularity in the late 1990s for text classification and bioinformatics.

«+ Became a standard tool with libraries like LIBSVM and scikit-learn.
25

Key Concepts

Support Vectors:
+ Data points closest to the hyperplane.

« They define the hyperplane's position and orientation.

Margin:
+ The distance between the hyperplane and the nearest support vector.

« SVM maximizes this margin for better generalization.

26


DA261 Machine Learning Fundamentals Course Instructor: Teena Sharma, Ph.D.

How SVM Works?

Step 1: Identify the optimal hyperplane that separates classes with the largest margin.
Step 2: Use support vectors to define the hyperplane.

Step 3: Classify new data points based on which side of the hyperplane they lie.

Example:
« Imagine separating apples and oranges on a table.

+ SVM finds the widest "gap" (margin) between the two groups.

27
ee

DA261 Machine Learning Fundamentals Course Instructor: Teena Sharma, Ph.D.

What is a hyperplane?

of
oe 7
«+ A hyperplane is a decision boundary separating e,e
- 6
classes. “0 686
+ In p-dimensional space, a hyperplane is a (p — 1) ~ 9 80

x

ce: H1 does not separate the classes.
H2 does, but only with a small margin. H3
separates them with the maximal margin.

>
L

In three-dimensional space, a two-
dimensional plane is a hyperplane.

dimensional affine subspace. 1D Subspa:

+ In 2D, a hyperplane is a flat 1D subspace, a line.

+ In 3D, a hyperplane is a flat 2D subspace, a plane.

28
ee Source: https. //en. wikipedia.org/wiki/Support_vector_machine


Course Instructor: Teena Sharma, Ph.D.

DA261 Machine Learning Fundamentals

Which Hyperplane?

+ Lots of possible solutions for a, b, c.

* Some methods find a separating hyperplane,
but not the optimal one (E.g., perceptron).

* Support Vector Machine (SVM) finds an optimal

solution.

o Maximizes the distance between the
hyperplane and the “difficult points” close

to the decision boundary

o One intuition: if there are no points near
the decision surface, then there are no

very uncertain classification decisions

This line represents the
decision boundary:
ax + by-c=0

29


DA261 Machine Learning Fundamentals Course Instructor: Teena Sharma, Ph.D.

Support Vector Machine (SVM)

Support vectors

SVMs maximize the margin around the
separating hyperplane, hence large margin
classifiers.

The decision function is fully specified by a
subset of training samples, the support vectors.

Solving SVMs is a quadratic programming
problem.

Seen by many as the most successful current Narrower Maximizes
ge . margin j
text classification method’. 8 margin

*but other discriminative methods often
perform very similarly

30


Maximum Margin: Formalization

* w: decision hyperplane normal vector
*  x,: data point i
+ y;! class of data point i (+1 or — 1)
* Classifier is: f(x;) = sign(w"x; + b)
* Functional margin of x; is: y; (w?x; + b)
o But note that we can increase this margin simply by scaling w, b ....
« Functional margin of dataset is twice the minimum functional margin for any point

o The factor of 2 comes from measuring the whole width of the margin

31


Geometric Margin

+ Distance from example to the separator is:

wlx+b
wll

« Examples closest to the hyperplane are support
vectors.

« Margin p of the separator is the width of
separation between support vectors of classes.

« Dotted line x’ — x is perpendicular to decision
w

boundary so parallel to w. Unit vector is Tw so

line is,
llwil

32


Linear SVM Mathematically

« Hyperplane is defined by the equation.
W'xX+b=0

+ Assume that all data is at least distance 1 from
the hyperplane, then the following two
constraints follow for a training set {(x; , y,)}

W?X,+b21 ify,;=1
W'X,+b<-1 ify;=-1

Maximum-margin hyperplane and margins for
an SVM trained with samples from two
+ For support vectors, the inequality becomes an classes. Samples on the margin are called the
. support vectors.
equality

33
e Source: https J/en. wikipedia.org/wiki/Support_vector_machine


Linear SVM Mathematically

e A better formulation (min ||w|]| = max 1/ ||w]|):

Find w and b such that

®(w) = %w'w is minimized;
and for all {yD yi (wW'x; + b) > 1

Maximum-margin hyperplane and margins for
an SVM trained with samples from two
classes. Samples on the margin are called the
support vectors.

34
ee Source: https. //en. wikipedia.org/wiki/Support_vector_machine


Soft Margin Classification

+ If the training data is not linearly separable,

slack variables ; can be added to allow
misclassification of difficult or noisy examples.
+ Allow some errors

o Let some points be moved to where they
belong, at a cost

« — Still, try to minimize training set errors, and to
place hyperplane “far” from each class (large
margin)

35


Soft Margin Classification Mathematically

«The old formulation:

Find w and b such that

®(w) = %w'w is minimized and for all {(xi , yi)}
y, wx, +b) 21

36

Soft Margin Classification Mathematically

«The old formulation:

Find w and b such that

®(w) = %w'w is minimized and for all {(xi , yi)}
y, wx, +b) 21

«The new formulation incorporating slack variables:

Find w and b such that

®(w) = %w' w + CYE; is minimized and for all {(xi , yi)}
y; W'x, + b) 21

« Parameter C can be viewed as a way to control overfitting

A regularization term

37

Classification with SVMs

+ Given a new point x, we can score its projection

onto the hyperplane normal:
o i.e., compute score: w"x + b
» Decide class based on whether < or > 0
o Can set confidence threshold t.

« Score > t: yes

» Score < -t:no

» Else: don’t know

38


Non-Linear SVMs

+ Datasets that are linearly separable (with some noise) work out great:

« But what if the dataset is too complex to separate linearly?

39


Non-Linear SVMs: Feature Space

* General idea: the original feature space can always be mapped to some higher-

dimensional feature space where the training set is separable:

40


iT] 7 3
The “Kernel Trick

* The linear classifier relies on an inner product between vectors K(x;,x;) = x} X;
+ If every datapoint is mapped into high-dimensional space via some transformation g: x >
(x), the inner product becomes: K(x;,x;) = 0(x:)" p(x)
« Akernel function is some function that corresponds to an inner product in some expanded
feature space.
«Example:
2-dimensional vectors x; = [xi1,x;2]; let K(x,.xj) = (1 + x!x;)°,

Need to show that K(x;,x)) = 9(x;)" @(x):

41


iT] 7 3
The “Kernel Trick

We being with K(x,,x;) = (1 + x?x;)’.
Assume x; = [xi xi2] and x; = [xj1, xj2| a 2D vector.
Then,

2 2
T _ _ 2,2 2,2
(4 + x; xj) = (1 + xi x51 + XinXj2) =1+ XX + 2X ir Xj Xi2Xjo + Xi2Xj2 + 2x11 + 2X j2Xj2

Now we ask, if there is a vector mapping (x) such that: K (x;,x,) = g(%i)" o(x;)
Let’s define: g(x) = [1,x?, x3, V2xyx2,V2x1, V2x|
Now take:
T
e(x;)' (x) = [1 xi xh V2xnx2 V2x_ V2xi2] [1 xf xp V2x;1%j2 V2xj. V2xj2]
9x)" e(x;) =1-1+ XX + 2x51 %)1%i2%j2 + Xp Xj + 2K 51551 + 2xj2X;2

Which exactly matches the expanded form of: (1 + x!x;)°

Hence, K(x,,x)) = g(x)" p(x).

42


iT] 7 3
The “Kernel Trick

«Why use kernels?

o Make non-separable problem separable.

o Map data into better representational space
* Common kernels

o Linear K(x,z) = x?z

o Polynomial K(x,z) = (1 +x"z)4

o Radial basis function (infinite dimensional space)

(e=xit")
K(x,x') = e\ 20?

43


261 Machine Learning Fundamentals Course Instruct eena Sharma, Ph.D.

Summary


DA261 Machine Learning Fundamentals Course Instructor: Teena Sharma, Ph.D.
Summary

* Decision Trees are intuitive, tree-structured models used for classification and regression
tasks. They recursively split the dataset based on feature values to form a hierarchy of decision
rules.

+ {D3 Algorithm: Uses Information Gain based on Entropy to select the best attribute at each
split, Prefers attributes that reduce uncertainty the most, May result in multiway splits (one
branch per attribute value).

*« CART Algorithm (Classification and Regression Tree): Uses Gini Index instead of entropy to
evaluate splits, Always creates binary trees - each split has exactly two branches, Applicable
for both classification and regression problems.

* Support Vector Machines (SVMs) are supervised learning models that find the optimal
hyperplane to classify data with the maximum margin, using kernels for handling non-linear
boundaries.

45

261 Machine Learning Fundamentals Course Instruct eena Sharma, Ph.D.

Thank You